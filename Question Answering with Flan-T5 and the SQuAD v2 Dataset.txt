Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f1EHH8Qq-QjqceV4tkBpT8aCSnmJY3IC

# Question Answering with Flan-T5 and SQuAD v2 Dataset
This notebook demonstrates how to perform question answering using a pre-trained Flan-T5 model fine-tuned on the SQuAD v2 dataset from Hugging Face.
"""

# Install necessary packages
!pip install transformers datasets tensorflow rouge-score nltk

"""## Import Libraries
We begin by importing the necessary libraries.
"""

import tensorflow as tf
from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq
from datasets import load_dataset

"""## Load the SQuAD v2 Dataset
Next, we load the SQuAD v2 dataset from Hugging Face.
"""

# Load the SQuAD v2 dataset
dataset = load_dataset("squad_v2")

"""## Load the Flan-T5 Tokenizer
We load the pre-trained Flan-T5 tokenizer to process the input text.
"""

# Load the Flan-T5 tokenizer
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")

"""## Tokenize the Dataset
We define a function to tokenize the dataset and apply it to the SQuAD v2 dataset.
"""

# Tokenize the dataset["train"].select(range(25000))
def preprocess_function(examples):
    inputs = [context + " question: " + question for question, context in zip(examples["question"], examples["context"])]
    targets = [answer['text'][0] if len(answer['text']) > 0 else "" for answer in examples["answers"]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=512, truncation=True, padding='max_length')

    model_inputs["labels"] = labels["input_ids"]
    model_inputs["decoder_input_ids"] = labels["input_ids"]
    return model_inputs

train_dataset = dataset["train"].select(range(25000)).map(preprocess_function, batched=True)
validation_dataset = dataset["validation"].select(range(2000)).map(preprocess_function, batched=True)

"""## Convert to TensorFlow Dataset
We convert the tokenized dataset to a format that can be used with TensorFlow.
"""

# Data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=None)

# Convert the tokenized dataset to a TensorFlow dataset
train_dataset = train_dataset.to_tf_dataset(
    columns=["input_ids", "attention_mask", "decoder_input_ids"],
    label_cols="labels",
    shuffle=True,
    batch_size=64,
    collate_fn=data_collator
)

validation_dataset = validation_dataset.to_tf_dataset(
    columns=["input_ids", "attention_mask", "decoder_input_ids"],
    label_cols="labels",
    shuffle=False,
    batch_size=64,
    collate_fn=data_collator
)

"""## Load and Configure the Flan-T5 Model
We load the pre-trained Flan-T5 model and configure it for sequence-to-sequence learning.
"""

# Load the pre-trained Flan-T5 model
model = TFAutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

"""## Freeze the First 3 Layers
We freeze the first three layers (embedding, encoder, decoder) to focus training on the head.
"""

# Freeze the first 3 layers (embedding, encoder, decoder)
for layer in model.layers[:3]:
    layer.trainable = False

model.summary()

"""## Compile the Model
We compile the model with appropriate loss function, optimizer, and metrics.
"""

# Compile the model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-2),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
)

"""## Train the Model
We train the model using the training dataset and validate it using the validation dataset.
"""

# Train the model
model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=3
)

"""## Save the Model
We save the trained model for future use.
"""

# Save the model
model.save_pretrained("./flan-t5-squad-v2")

"""## Evaluate the Model
Finally, we evaluate the model using the validation dataset to check its performance using ROUGE-1 score.
"""

from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import nltk
nltk.download('punkt')

def answer(inputs):
    outputs = model.generate(inputs[0]["input_ids"], max_length=128, num_beams=4, early_stopping=True)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


# Function to calculate ROUGE and BLEU scores
def calculate_scores(reference, hypothesis):
    # Initialize scorers
    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    # Calculate ROUGE scores
    rouge_scores = rouge.score(reference, hypothesis)

    return rouge_scores

# Evaluate translations and calculate scores
batch = next(iter(validation_dataset))
answer = answer(batch)
reference_text = tokenizer.decode(batch[1][0], skip_special_tokens=True)
rouge_scores = calculate_scores(reference_text, answer)
print(f"Reference: {reference_text}")
print(f"Translation: {answer}")
print(f"ROUGE Scores: {rouge_scores}")
print()

